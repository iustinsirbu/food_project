{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This could be moved into a .py file and imported in both notebooks\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MyNER(tf.keras.Model):\n",
    "    def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, embed_weights, num_labels):\n",
    "        super(MyNER, self).__init__() \n",
    "        self.embedding = layers.Embedding(input_dim=embed_input_dim, \n",
    "        output_dim=embed_output_dim, weights=embed_weights,    \n",
    "        input_length=max_seq_len, trainable=False, mask_zero=True)\n",
    "\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))\n",
    "        self.dense = layers.Dense(num_labels)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs) # batchsize, max_seq_len, embedding_output_dim\n",
    "        \n",
    "        x = self.bilstm(x) # batchsize, max_seq_len, hidden_dim_bilstm\n",
    "        \n",
    "        logits = self.dense(x) #batchsize, max_seq_len, num_labels\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bilstm_model_0/settings.pkl', 'rb') as f_in:\n",
    "    settings = pickle.load(f_in)\n",
    "    \n",
    "with open('bilstm_model_0/mappings.pkl', 'rb') as f_in:\n",
    "    mappings = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa1dc28e4f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = MyNER(**settings)\n",
    "test_model.load_weights(f\"bilstm_model_0/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModel():\n",
    "    def __init__(self, model, mappings, settings):\n",
    "        self.model = model\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.settings = settings\n",
    "        self.mappings = mappings\n",
    "        self.word2id = self.mappings['word2id']\n",
    "        self.id2label = self.mappings['id2label']\n",
    "        self.batch_size = 2 # this may be different from the one used at training, depending on the server resources\n",
    "        \n",
    "    def create_features(self, word_sentences):\n",
    "        id_sentences = []\n",
    "        for word_sentence in word_sentences:\n",
    "            word_indices = []\n",
    "            for word in word_sentence:\n",
    "                if word.lower() in self.word2id:\n",
    "                    word_idx = self.word2id[word.lower()]\n",
    "                else:\n",
    "                    word_idx = self.word2id['UnkWord']\n",
    "                word_indices.append(word_idx)\n",
    "            id_sentences.append(word_indices)\n",
    "        \n",
    "        padded_sentences = pad_sequences(id_sentences, self.settings['max_seq_len'], padding='post')\n",
    "        return padded_sentences\n",
    "    \n",
    "    def indices_to_labels(self, indices):\n",
    "        labels = []\n",
    "        for batch in indices:\n",
    "            for sentence in batch:\n",
    "                label = [self.id2label[idx] for idx in sentence]\n",
    "                labels.append(label)\n",
    "        return labels\n",
    "    \n",
    "    def format_response(self, row):\n",
    "        response = []\n",
    "        for i in range(len(row.words)):\n",
    "            if row.pred_labels[i] != 'B-INGREDIENT':\n",
    "                continue\n",
    "            start = row.tokens[i].idx\n",
    "            j = i + 1\n",
    "            while j < len(row.words) and row.pred_labels[j] != 'O':\n",
    "                j += 1\n",
    "            # j-1 is the last token from the current INGREDIENT\n",
    "            end = row.tokens[j-1].idx + len(row.tokens[j-1].text)\n",
    "            ingredient = row.Directions[start:end]\n",
    "            response.append([ingredient, start, end])\n",
    "        return response\n",
    "        \n",
    "    def predict(self, request):\n",
    "        df_request = pd.DataFrame(request.items(), columns=['Recipe Name', 'Directions'])\n",
    "        df_request['texts'] = df_request.Directions.map(lambda x: x if pd.notna(x) else '')\n",
    "        df_request['docs'] = df_request.texts.map(lambda text: self.nlp(text))\n",
    "        df_request['tokens'] = df_request.docs.map(lambda doc: [token for token in doc])\n",
    "        df_request['words'] = df_request.docs.map(lambda doc: [token.text for token in doc])\n",
    "        \n",
    "        features = self.create_features(df_request.words)\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "        dataset = dataset.batch(self.batch_size, drop_remainder=False)\n",
    "        \n",
    "        pred_labels = []\n",
    "        for sentences_batch in dataset:\n",
    "            logits = self.model(sentences_batch)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            preds = tf.argmax(probs, axis=2)\n",
    "            pred_labels.append(np.asarray(preds))\n",
    "        \n",
    "        df_request['pred_labels'] = self.indices_to_labels(pred_labels)\n",
    "        df_request['response'] = df_request.apply(self.format_response, axis=1)\n",
    "        \n",
    "        response_dict = {}\n",
    "        for _, row in df_request.iterrows():\n",
    "            response_dict[row['Recipe Name']] = row.response\n",
    "        \n",
    "        return response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "req = {\n",
    "    \"recipe1\": \"This is recipe 1 owdquyvgk\",\n",
    "    \"recipe2\": \"This is recipe 2 directions\",\n",
    "    \"Cream of Cauliflower Soup II Recipe\": 'In a large pot over medium heat, melt butter.  Stir in onion and garlic and cook until onion is translucent, about 5 minutes.  Stir in potatoes and carrots and cook 5 minutes more.  Pour in chicken broth and bring to a boil.  Stir in cauliflower, cover, reduce heat and simmer until vegetables are tender, 10 to 20 minutes.  Remove from heat.**Puree in batches in a blender or food processor, or in the pot using an immersion blender.  Return to low heat and stir in milk, salt, pepper, nutmeg and sherry.  Heat through.  Serve garnished with parsley.**',\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = InferenceModel(test_model, mappings, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = inference_model.predict(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe1': [],\n",
       " 'recipe2': [],\n",
       " 'Cream of Cauliflower Soup II Recipe': [['butter', 38, 44],\n",
       "  ['onion', 55, 60],\n",
       "  ['garlic', 65, 71],\n",
       "  ['onion', 87, 92],\n",
       "  ['potatoes', 135, 143],\n",
       "  ['carrots', 148, 155],\n",
       "  ['chicken broth', 190, 203],\n",
       "  ['cauliflower', 234, 245],\n",
       "  ['milk', 467, 471],\n",
       "  ['salt', 473, 477],\n",
       "  ['nutmeg', 487, 493],\n",
       "  ['sherry', 498, 504],\n",
       "  ['parsley', 543, 550]]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yolo_env]",
   "language": "python",
   "name": "conda-env-yolo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
